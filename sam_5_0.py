# -*- coding: utf-8 -*-
"""sam_5_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sYoaSncUU-1NwYgQWVi4LTfBSnm_9t3v
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout, Conv2D,Lambda, Conv2DTranspose, MaxPooling2D,ZeroPadding2D, UpSampling2D, Input, Reshape, GlobalAveragePooling2D, Concatenate,MaxPool2D
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import layers
import tensorflow as tf
import numpy as np
import pandas as pd
import glob
import PIL
from PIL import Image
import matplotlib.pyplot as plt
import cv2
# %matplotlib inline
import tensorflow.keras
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from warnings import filterwarnings

filterwarnings('ignore')
plt.rcParams["axes.grid"] = False
np.random.seed(101)

!pip install tensorflow_addons

#@title image encoder
from tensorflow.keras import layers
import tensorflow_addons as tfa
from tensorflow import keras
import tensorflow as tf
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
import random

# DATA


# ENCODER and DECODER
LAYER_NORM_EPS = 1e-6
ENC_PROJECTION_DIM = 128
DEC_PROJECTION_DIM = 64
ENC_NUM_HEADS = 4
ENC_LAYERS = 1
ENC_TRANSFORMER_UNITS = [
    ENC_PROJECTION_DIM * 2,
    256,
]  # Size of the transformer layers.



def pad_tensor(input_tensor, pad_size):
    pad = tf.constant([[0, 0], [0,0], [0, pad_size], [0, 0]])
    padded_tensor = tf.pad(input_tensor, pad, mode='CONSTANT')
    return padded_tensor

def pooled(input):
      # pad_size = 1
      # input = pad_tensor(input,pad_size)
      pooling = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(input)
      return pooling
def convolutional_block(input, filters):
    conv1 = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same', activation='relu')(input)
    bn1 = tf.keras.layers.BatchNormalization()(conv1)
    relu1 = tf.keras.layers.ReLU()(bn1)
    conv2 = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same', activation='relu')(relu1)
    bn2 = tf.keras.layers.BatchNormalization()(conv2)
    relu2 = tf.keras.layers.ReLU()(bn2)
    return relu2

def decoder_block(input, skip_features, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = convolutional_block(x, num_filters)
    return x

def norm(input):

  input = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(input)
  input = convolutional_block(input,128)
  input = pooled(input)
  input = convolutional_block(input,256)
  input = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(input)
  input = convolutional_block(input,512)
  input =pooled(input)
  input = convolutional_block(input,1024)

  return input

def feedBackLoop(input,conv4,conv3,conv2,conv1,n):
   for i in range(n):
    d1 = decoder_block(input, conv4 , 512)
    d2 = decoder_block(d1, conv3, 256)
    d3 = decoder_block(d2, conv2, 128)
    d4 = decoder_block(d3, conv1, 64)
   return d4

def create(inputs):
    num_heads=ENC_NUM_HEADS
    num_layers=ENC_LAYERS
    pad_size = 15
    x = inputs
    # conv1= tf.pad(x, paddings=( (0, 0), (0, 1)))
    conv1 = convolutional_block(inputs, filters=64)
    conv1 = pad_tensor(conv1,pad_size)
    print(conv1.shape)
    pool1= pooled(conv1)
    pool1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(pool1)

    attention_output =tf.keras.layers.Attention()([pool1,pool1])

    pool1 = layers.Add()([attention_output, pool1])

    conv2 = convolutional_block(pool1, filters=128)
    pool2= pooled(conv2)
    pool2 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(pool2)
    x2 = tf.keras.layers.Attention()([pool2,pool2])
    # x2= layers.MultiHeadAttention(
    #         num_heads=num_heads, key_dim=ENC_PROJECTION_DIM, dropout=0.1
    #     )(pool2,pool2)
    pool2 = layers.Add()([x2, pool2])

    conv3 = convolutional_block(pool2, filters=256)
    pool3= pooled(conv3)
    x3 = tf.keras.layers.Attention()([pool3,pool3])
    # x3 = layers.MultiHeadAttention(
    #         num_heads=num_heads, key_dim=ENC_PROJECTION_DIM, dropout=0.1
    #     )(pool3,pool3)
    pool3 = layers.Add()([x3, pool3])

    conv4 = convolutional_block(pool3, filters=512)
    pool4= pooled(conv4)
    x4 =tf.keras.layers.Attention()([pool4,pool4])
    #  layers.MultiHeadAttention(
    #         num_heads=num_heads, key_dim=ENC_PROJECTION_DIM, dropout=0.1
    #     )(pool4,pool4)
    pool4 = layers.Add()([x4, pool4])#######s4



    x= convolutional_block(pool4,filters=1024)####b4
    print(pool4.shape)
    print(x.shape)

    d1 = decoder_block(x, conv4 , 512)
    d2 = decoder_block(d1, conv3, 256)
    d3 = decoder_block(d2, conv2, 128)
    d4 = decoder_block(d3, conv1, 64)
    d4 = norm(d4)
    d4 = layers.Add()([x, d4])

    d4 = feedBackLoop(d4,conv4,conv3,conv2,conv1,4)
    print(d4.shape)
    d4 = norm(d4)
    d4 = convolutional_block(d4, filters=128)
    d4 = tf.reshape(d4,(-1,16,16,375),name = None)

    d4 =  convolutional_block(d4, filters=1)
#     d4= tf.reshape(
#     d4,(25,30,1), name=None
# )


    outputs = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(d4)
    outputs = tf.keras.activations.sigmoid(outputs)

    return outputs

def sam():
    inputs = layers.Input((2000, 81, 2))
    outputs = create(inputs)  # Assuming you've defined create_encoder function
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

# Assuming you've defined the necessary functions

model = sam()
model.summary()

